{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e60ac47",
   "metadata": {},
   "source": [
    "# ETL I: Data Extraction with Web Scrapping\n",
    "\n",
    "<br>\n",
    "\n",
    "<img width=80 src=\"https://media.giphy.com/media/KAq5w47R9rmTuvWOWa/giphy.gif\">\n",
    "\n",
    "<img width=150 src=\"../Images/assblr.png\">\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e52583",
   "metadata": {},
   "source": [
    "## Scrapping data from the web\n",
    "\n",
    "Sometimes we have data displayed on the web and that website don't have a download function or an api to access to the content, so the option we have is to read the content \"As a user\" and get that content.\n",
    "\n",
    "To do this we'll use the urllib library and beautiful soup.\n",
    "\n",
    "Imagine we need to extract the filmography of Anne Hathaway and we found this information in wikipedia (https://es.wikipedia.org/wiki/Anne_Hathaway), let's see the process to get the data.\n",
    "\n",
    "### Getting the raw data\n",
    "\n",
    "To get the raw data we need to make a call to the website we want to get. The best way to not be blocked is to use our firefox headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2c319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "\n",
    "webpage = 'https://es.wikipedia.org/wiki/Anne_Hathaway'\n",
    "\n",
    "req = Request(webpage, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "raw_web = urlopen(req, timeout=10).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cb915",
   "metadata": {},
   "source": [
    "With this, we have the whole HTML of the website in the `raw_web`\n",
    "variable.\n",
    "\n",
    "Here's the first 400 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49645658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-enabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled vector-feature-zebra-design'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_web[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c84231",
   "metadata": {},
   "source": [
    "### Looking for the data we need\n",
    "\n",
    "Now we have the content, we need to clean everything we don't want. To do this, first we need to know where is the data we want, the easiest way to do it, is opening the website in a new tab, right-click in the content we want and inspect it.\n",
    "\n",
    "Once we do that, we can see that the content is under a table tag like this:\n",
    "`<table class=\"wikitable sortable jquery-tablesorter\"><table>`\n",
    "\n",
    "We'll use beautful soup library to get the table data (docs:\n",
    "<https://www.crummy.com/software/BeautifulSoup/bs4/doc/>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d0615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(raw_web, 'html.parser')\n",
    "tables = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a420041",
   "metadata": {},
   "source": [
    "Whith the command above we have extracted all tables in the website. \n",
    "\n",
    "But if we have a look on how many tables we can see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc68ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f6688",
   "metadata": {},
   "source": [
    "We can try another way of extracting, limiting the results to the class we saw when inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865a9fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = soup.find_all('table', attrs={\"class\": \"wikitable sortable\"})\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae991cef",
   "metadata": {},
   "source": [
    "Now we only have two tables and we can explore them. We'll work with the first one only by at this moment.\n",
    "\n",
    "First we need to get the headers and rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c1a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_head = tables[0].find_all('th')[1:] #We skip the first header as we saw we don't want it\n",
    "table_rows = tables[0].find_all('tr')[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c9300",
   "metadata": {},
   "source": [
    "Once we have that data, we need to clean the html to get only the content and discard the html. We'll do it also with the beautifulsoup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa1ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "theads = []\n",
    "trows =[]\n",
    "\n",
    "for col in table_head:\n",
    "    theads.append(col.text.strip()) #With the strip() method we can delete the (\\n) new line character\n",
    "    \n",
    "for row in table_rows:\n",
    "    content = []\n",
    "    for col in row:\n",
    "        content.append(col.text.strip())\n",
    "    try:\n",
    "        content = [content[1], content[3], content[5], content[7]] #We select only the columns we need\n",
    "    except:\n",
    "        error = 'ignore' #We add this line just to ignore the error that is normal in our case\n",
    "    \n",
    "    trows.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810fe727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table headers: ['Año', 'Título original', 'Papel', 'Notas']\n",
      "Table first row ['2001', 'The Princess Diaries', 'Mia Thermopolis', '']\n"
     ]
    }
   ],
   "source": [
    "print('Table headers:', theads)\n",
    "print('Table first row', trows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33528e69",
   "metadata": {},
   "source": [
    "Let's save this data to use it later with the open command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d96f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sources/Anne_Hathaway.txt', 'w') as file:\n",
    "    \n",
    "    content = '' #We create an empty content\n",
    "    \n",
    "    content = content + ','.join(theads) + '\\n' #we add the heads as the first line\n",
    "    \n",
    "    for row in trows:\n",
    "        content = content + ','.join(row)+'\\n' #This adds each row as a line        \n",
    "    \n",
    "    file.write(content) #This writes the content in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da14ee",
   "metadata": {},
   "source": [
    "This is what you need to extract the most types of data you'll work with. Remember also, you can get data from other sources, for example an SQL database as you saw in the SQL lesson (you'll practice with this source in the next class)\n",
    "\n",
    "You'll need to pay attention to every source you want to connect to because they could be all different and have some particularities you'll need to solve. But don't worry, practice and experience will let you solve any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f91cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
